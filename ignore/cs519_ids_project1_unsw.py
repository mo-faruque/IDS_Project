# -*- coding: utf-8 -*-
"""CS519_IDS_project1_UNSW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MYxFJ2hzvUMH6Il0nQMwbE4PDPpFKjsx
"""

# Commented out IPython magic to ensure Python compatibility.
#Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# LazyClassifier for quick model benchmarking
# pip install lazypredict               # Install if necessary/Giving Error while importing LazyClassifier
from lazypredict.Supervised import LazyClassifier

# === PCA with RandomForest Pipeline ===

from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve


# ANN + SMOTE
#!pip install tensorflow     #install if needed

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# pip install scikit-fuzzy # install if necessary
import skfuzzy as fuzz
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from scipy.stats import mode

# For scatter matrix plots
from pandas.plotting import scatter_matrix

# For explainability analysis
import shap
import pickle

# Load the pre-split training and testing datasets
train_path = "UNSW_NB15_training-set.csv"
test_path = "UNSW_NB15_testing-set.csv"

df_train = pd.read_csv(train_path)
df_test = pd.read_csv(test_path)

# Combine for unified processing (optional)
df = pd.concat([df_train, df_test], ignore_index=True)
# Reduce sample size for quicker execution
df = df.sample(n=20000, random_state=42).copy()

# Drop 'attack_cat' column as requested
if 'attack_cat' in df.columns:
    print("Dropping 'attack_cat' column.")
    df = df.drop(columns=['attack_cat'])
else:
    print("'attack_cat' column not found.")

# Preview dataset
print("\nDataset Head after dropping 'attack_cat':")
print(df.head())

#Basic EDA
print("=== Dataset Info ===")
df.info()

print("\n=== Statistical Summary ===")
print(df.describe())

print("\n=== Missing Values per Column ===")
print(df.isnull().sum())


print("\n=== Target Variable Distribution ===")
print(df['label'].value_counts(normalize=True))

# Advanced EDA
# Settting target column name here
target_col = 'label'

# Identify numeric and categorical features (excluding the target)
numeric_features = df.select_dtypes(include=['int64','float64']).columns.tolist()
categorical_features = df.select_dtypes(include=['object']).columns.tolist()
if target_col in numeric_features:
    numeric_features.remove(target_col)
if target_col in categorical_features:
    categorical_features.remove(target_col)

# Distribution plots for numeric features
for col in numeric_features:
    plt.figure()
    plt.hist(df[col].dropna(), bins=30, edgecolor='k')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

# Boxplots for numeric features to detect outliers
for col in numeric_features:
    plt.figure()
    plt.boxplot(df[col].dropna(), vert=False)
    plt.title(f'Boxplot of {col}')
    plt.xlabel(col)
    plt.tight_layout()
    plt.show()

# Bar plot for target variable distribution
plt.figure()
df[target_col].value_counts().plot(kind='bar')
plt.title('Target Distribution')
plt.xlabel(target_col)
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# Correlation matrix heatmap for numeric features
if numeric_features:
    corr = df[numeric_features].corr()
    plt.figure(figsize=(10, 8))
    plt.imshow(corr, cmap='coolwarm', interpolation='none', aspect='auto')
    plt.colorbar()
    plt.xticks(range(len(corr)), corr.columns, rotation=90)
    plt.yticks(range(len(corr)), corr.columns)
    plt.title("Correlation Matrix")
    plt.tight_layout()
    plt.show()

# Scatter matrix (if the number of features is reasonable)
if len(numeric_features) <= 10 and numeric_features:
    scatter_matrix(df[numeric_features], figsize=(12,12), diagonal='kde')
    plt.suptitle('Scatter Matrix of Numeric Features')
    plt.tight_layout()
    plt.show()

#Data Preprocessing

# Separate features and target
X = df.drop(target_col, axis=1)
y = df[target_col]

# --- Explicitly remove potential ID / high-cardinality columns ---
# List of columns to potentially drop (adjust based on actual dataset columns)
cols_to_drop_ids = ['id']

# Drop columns if they exist in X
existing_cols_to_drop = [col for col in cols_to_drop_ids if col in X.columns]
if existing_cols_to_drop:
    print(f"Dropping ID column(s): {existing_cols_to_drop}")
    X = X.drop(columns=existing_cols_to_drop)
else:
    print("ID column not found to drop.")
# ------------------------------------------------------------------

# Identify numeric and categorical columns from the *modified* X
print("\nColumns in X after dropping 'id':")
print(X.info())
numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
print("Numeric features:", numeric_features)
print("Categorical features:", categorical_features)

# Create transformers for numeric and categorical data
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])

# Build the preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

#LazyClassifier Analysis

# LazyClassifier requires all numeric inputs. Convert categorical features using get_dummies.
X_dummy = pd.get_dummies(X)
X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy = train_test_split(
    X_dummy, y, test_size=0.2, random_state=42, stratify=y
)

clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy)
# print(models)
print(models.sort_values(by="Accuracy", ascending=False))


# # Preprocess data
# X_processed = preprocessor.fit_transform(X)
# feature_names = preprocessor.get_feature_names_out()

# # Convert to dense DataFrame
# X_processed_df = pd.DataFrame(X_processed.toarray(), columns=feature_names)

# # Split data
# X_train, X_test, y_train, y_test = train_test_split(
#     X_processed_df, y, test_size=0.2, random_state=42, stratify=y
# )

# # Verify data
# assert X_train.select_dtypes(exclude='number').empty, "Non-numeric data found!"
# assert not X_train.isna().any().any(), "NaNs found!"
# assert not np.isinf(X_train.to_numpy()).any(), "Infinite values found!"

# # Run LazyClassifier
# clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
# models, _ = clf.fit(X_train, X_test, y_train, y_test)

# print(models.sort_values(by="Accuracy", ascending=False))

#Split Data for Hyperparameter Tuning
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Hyperparameter Tuning on RandomForest
# Build a pipeline with the preprocessor and RandomForestClassifier
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Set up the parameter grid for GridSearchCV
param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20, 30],
}

grid_search = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best Parameters Found:", grid_search.best_params_)
print("Best Cross-validation Accuracy: {:.4f}".format(grid_search.best_score_))
best_model = grid_search.best_estimator_

# Evaluate the Tuned Model
y_pred = best_model.predict(X_test)
print("=== Classification Report ===")
print(classification_report(y_test, y_pred))

print("Accuracy:", accuracy_score(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
print("=== Confusion Matrix ===")
print(cm)

plt.figure(figsize=(6,6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# If binary classification, plot the ROC curve.
if len(np.unique(y_test)) == 2:
    y_proba = best_model.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    roc_auc = roc_auc_score(y_test, y_proba)
    plt.figure()
    plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.show()

# === PCA with RandomForest Pipeline ===
#Applying PCA to reduce dimensionality, minimize redundancy, and improve efficiency

# Define PCA and classifier
pca = PCA(n_components=10)
rf_clf = RandomForestClassifier(random_state=42)

# Build pipeline: preprocessing → PCA → classifier
pca_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('pca', pca),
    ('classifier', rf_clf)
])

# Fit pipeline
pca_pipeline.fit(X_train, y_train)

# Predict and evaluate
y_pred = pca_pipeline.predict(X_test)
print("\n=== PCA + RandomForest Classification Report ===")
print(classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("=== Confusion Matrix ===")
print(cm)

plt.figure(figsize=(6,6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix (PCA)')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# ROC Curve (for binary classification)
if len(np.unique(y_test)) == 2:
    y_proba = pca_pipeline.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    roc_auc = roc_auc_score(y_test, y_proba)

    plt.figure()
    plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve (PCA)')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.show()

# === ANN + SMOTE ===
if preprocessor is not None and X_train is not None:
    print("\n--- ANN + SMOTE Pipeline --- ")
    # Define the ANN model structure
    def create_ann_model(input_shape):
        model = Sequential()
        model.add(Dense(64, input_dim=input_shape, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(32, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(1, activation='sigmoid')) # Binary output
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

    try:
        # 1. Preprocess training and test data using the *already fitted* preprocessor
        # Preprocessor was fitted on X with session_id dropped
        X_train_processed = preprocessor.transform(X_train)
        X_test_processed = preprocessor.transform(X_test)
        input_dim_ann = X_train_processed.shape[1]
        print(f"Input dimension for ANN: {input_dim_ann}")

        # 2. Apply SMOTE only to the processed training data
        smote = SMOTE(random_state=42)
        print(f"Shape before SMOTE: {X_train_processed.shape}, Class distribution: {np.bincount(y_train)}")
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)
        print(f"Shape after SMOTE: {X_train_balanced.shape}, Class distribution: {np.bincount(y_train_balanced)}")

        # 3. Create and Train the ANN model on balanced data
        ann_model = create_ann_model(input_dim_ann)
        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
        history = ann_model.fit(
            X_train_balanced, y_train_balanced,
            validation_split=0.2,
            epochs=30,
            batch_size=64,
            callbacks=[early_stop],
            verbose=1
        )

        # 4. Evaluate the model on the processed test set
        y_pred_prob_ann = ann_model.predict(X_test_processed)
        y_pred_ann = (y_pred_prob_ann > 0.5).astype("int32")

        print("\n=== ANN + SMOTE Results ===")
        print("Accuracy (ANN + SMOTE): {:.4f}".format(accuracy_score(y_test, y_pred_ann)))
        print("\nConfusion Matrix (ANN + SMOTE):\n", confusion_matrix(y_test, y_pred_ann))
        print("\nClassification Report (ANN + SMOTE):\n", classification_report(y_test, y_pred_ann))

        # Plot training history
        pd.DataFrame(history.history).plot(figsize=(8, 5))
        plt.grid(True)
        plt.gca().set_ylim(0, 1)
        plt.title('ANN Model Training History')
        plt.show()

    except Exception as e:
        print(f"Error during ANN + SMOTE pipeline: {e}")
        import traceback
        traceback.print_exc()
else:
    print("Preprocessor or training data not available. Skipping ANN + SMOTE.")

#=== Fuzzy C-Means ===

# Standardize only numeric features
X_fcm = df[numeric_features].copy()
X_scaled = StandardScaler().fit_transform(X_fcm)

# Fuzzy C-Means Clustering
n_clusters = 2  # Since IDS is binary (attack or normal)
cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(
    X_scaled.T, c=n_clusters, m=2, error=0.005, maxiter=1000
)

# Assign cluster labels based on max membership
fcm_labels = np.argmax(u, axis=0)

# Map cluster labels to actual class labels using majority voting
cluster_to_label = {}
for cluster in range(n_clusters):
    # Get the mode and handle cases where it's a scalar
    mode_result = mode(y[fcm_labels == cluster])
    # Check if mode_result.mode is an array or scalar
    majority_class = mode_result.mode[0] if isinstance(mode_result.mode, np.ndarray) and mode_result.count[0] > 0 else mode_result.mode if mode_result.count > 0 else y[fcm_labels == cluster].iloc[0] #If mode is empty, using first element.
    # If mode_result.mode is a scalar, use it directly; otherwise, access the first element
    cluster_to_label[cluster] = majority_class

# Convert fuzzy cluster labels to predicted class labels
fcm_pred = np.array([cluster_to_label[label] for label in fcm_labels])

# Add to DataFrame (optional)
df['fuzzy_cluster'] = fcm_labels
df['fcm_predicted_class'] = fcm_pred

# Evaluate
print("=== Fuzzy C-Means Results ===")
print("Accuracy:", accuracy_score(y, fcm_pred))
print("Confusion Matrix:\n", confusion_matrix(y, fcm_pred))
print("Classification Report:\n", classification_report(y, fcm_pred))

#=== Fuzzy C-Means with Hyperparameter tuning ===

# Standardize only numeric features
X_fcm = df[numeric_features].copy()
X_scaled = StandardScaler().fit_transform(X_fcm)

# Fuzzy C-Means Clustering with n_clusters and m tuning
n_clusters_range = [2, 3, 4, 5]
m_range = [1.5, 1.7, 1.8, 2.0]

best_accuracy = 0
best_params = (None, None)

# Loop through different n_clusters and m values
for n in n_clusters_range:
    for m in m_range:
        # Perform fuzzy clustering
        cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(X_scaled.T, c=n, m=m, error=0.005, maxiter=1000)
        fcm_labels = np.argmax(u, axis=0)  # Assign clusters based on max membership

        # Create cluster_to_label mapping for this iteration
        cluster_to_label = {}
        for cluster in range(n):  # Use 'n' (number of clusters) here
            mode_result = mode(y[fcm_labels == cluster])
            majority_class = mode_result.mode[0] if isinstance(mode_result.mode, np.ndarray) and mode_result.count[0] > 0 else mode_result.mode if mode_result.count > 0 else y[fcm_labels == cluster].iloc[0]
            cluster_to_label[cluster] = majority_class

        fcm_pred = np.array([cluster_to_label[label] for label in fcm_labels])

        # Compute accuracy
        accuracy = accuracy_score(y, fcm_pred)

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = (n, m)

print(f"Best n_clusters: {best_params[0]}, Best m: {best_params[1]}, Accuracy: {best_accuracy}")


# Re-run the best configuration with the final clustering
n_clusters = best_params[0]
m = best_params[1]
cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(X_scaled.T, c=n_clusters, m=m, error=0.005, maxiter=1000)

# Assign cluster labels based on max membership
fcm_labels = np.argmax(u, axis=0)

# Map cluster labels to actual class labels using majority voting
cluster_to_label = {}
for cluster in range(n_clusters):
    mode_result = mode(y[fcm_labels == cluster])
    majority_class = mode_result.mode[0] if isinstance(mode_result.mode, np.ndarray) and mode_result.count[0] > 0 else mode_result.mode if mode_result.count > 0 else y[fcm_labels == cluster].iloc[0]
    cluster_to_label[cluster] = majority_class

# Convert fuzzy cluster labels to predicted class labels
fcm_pred = np.array([cluster_to_label[label] for label in fcm_labels])

# Add to DataFrame (optional)
df['fuzzy_cluster'] = fcm_labels
df['fcm_predicted_class'] = fcm_pred

# Evaluate performance
print("=== Fuzzy C-Means Results ===")
print("Accuracy:", accuracy_score(y, fcm_pred))
print("Confusion Matrix:\n", confusion_matrix(y, fcm_pred))
print("Classification Report:\n", classification_report(y, fcm_pred))

# Optionally, also evaluate the silhouette score
from sklearn.metrics import silhouette_score
sil_score = silhouette_score(X_scaled, fcm_labels)
print(f"Silhouette Score: {sil_score}")

import shap
import numpy as np
from sklearn.model_selection import train_test_split

# # Keeping this Split code here as sometimes need to  run  as further code was giving issue
# # Split Data for Hyperparameter Tuning
# X_train, X_test, y_train, y_test = train_test_split(
#     df.drop(target_col, axis=1),
#     y,
#     test_size=0.2,
#     random_state=42,
#     stratify=y
# )

# Transform the training set using the pipeline preprocessor (fit and transform)
X_train_transformed = best_model.named_steps['preprocessor'].fit_transform(X_train)

# Transform the test set using the pipeline preprocessor (only transform)
X_test_transformed = best_model.named_steps['preprocessor'].transform(X_test)

# Ensure that both X_train_transformed and X_test_transformed have the correct format
if hasattr(X_train_transformed, "toarray"):
    X_train_transformed = X_train_transformed.toarray()  # Convert sparse matrix to dense
if hasattr(X_test_transformed, "toarray"):
    X_test_transformed = X_test_transformed.toarray()  # Convert sparse matrix to dense

X_train_transformed = X_train_transformed.astype(np.float64)
X_test_transformed = X_test_transformed.astype(np.float64)

# Feature names after preprocessing (assuming you're using one-hot encoding for categorical features)
try:
    # Get feature names for categorical features if available
    cat_feature_names = best_model.named_steps['preprocessor'].named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)
except AttributeError:
    # If the model doesn't have 'get_feature_names_out', use the older method
    cat_feature_names = best_model.named_steps['preprocessor'].named_transformers_['cat']['onehot'].get_feature_names(categorical_features)

# Combine numeric features and categorical features for the feature names
feature_names = numeric_features + list(cat_feature_names)

# Sample for faster SHAP analysis (just using the first 100 samples for speed)
X_sample = X_test_transformed[:100]  # Adjust the number based on your system's capacity

# === SHAP Explainability Analysis (using best RandomForest model 'best_model') ===
if best_model is not None and X_test is not None and preprocessor is not None:
    print("\n--- SHAP Analysis for Tuned RandomForest Model --- ")
    try:
        # 1. Get the fitted preprocessor and classifier from the best RF pipeline
        fitted_preprocessor = best_model.named_steps['preprocessor']
        classifier_shap = best_model.named_steps['classifier']

        # 2. Transform the test set using the *same* preprocessor
        X_test_transformed_shap = fitted_preprocessor.transform(X_test)
        print(f"Shape of X_test_transformed_shap after preprocessor: {X_test_transformed_shap.shape}")
        print(f"Type of X_test_transformed_shap after preprocessor: {type(X_test_transformed_shap)}")

        # 3. Get feature names after preprocessing
        processed_feature_names_shap = None
        try:
            # Get numeric and categorical feature names *as used by the fitted preprocessor*
            numeric_features_tf = fitted_preprocessor.transformers_[0][2]
            categorical_features_tf = fitted_preprocessor.transformers_[1][2]
            ohe_transformer = fitted_preprocessor.named_transformers_['cat']['onehot']
            cat_feature_names_shap = ohe_transformer.get_feature_names_out(categorical_features_tf)
            processed_feature_names_shap = list(numeric_features_tf) + list(cat_feature_names_shap)
            print(f"Number of features for SHAP: {len(processed_feature_names_shap)}")
        except Exception as e:
            print(f"Could not retrieve feature names for SHAP: {e}. Using generic names.")
            num_processed_features = X_test_transformed_shap.shape[1]
            processed_feature_names_shap = [f'feature_{i}' for i in range(num_processed_features)]

        # 4. Create SHAP Explainer
        explainer = shap.TreeExplainer(classifier_shap)

        # 5. Calculate SHAP values (use a sample for speed)
        # Convert transformed data to DataFrame with feature names for SHAP
        X_test_transformed_df_shap = pd.DataFrame(X_test_transformed_shap, columns=processed_feature_names_shap)

        sample_size_shap = min(100, X_test_transformed_df_shap.shape[0])
        if sample_size_shap > 0:
            X_sample_shap = X_test_transformed_df_shap.sample(sample_size_shap, random_state=42)
            print(f"Calculating SHAP values for {sample_size_shap} samples...")
            # Calculate SHAP values
            shap_values = explainer.shap_values(X_sample_shap)
            print(f"Shape of shap_values: {np.shape(shap_values)}") # Debug print

            # --- Calculate and Print SHAP Feature Importance ---
            shap_values_for_importance = None
            shap_values_for_plot = None
            plot_title = "SHAP Summary Plot"

            # Check the structure of shap_values
            if isinstance(shap_values, list) and len(shap_values) == 2:
                # Handles case where output is a list [shap_class_0, shap_class_1]
                print("SHAP values format: List of arrays per class.")
                shap_values_for_importance = shap_values[1] # Use class 1 for importance
                shap_values_for_plot = shap_values[1]
                plot_title = "SHAP Summary Plot (Class 1 - Attack)"
            elif isinstance(shap_values, np.ndarray) and shap_values.ndim == 3 and shap_values.shape[-1] == 2:
                # Handles case where output is a single array (samples, features, classes)
                print("SHAP values format: Single array (samples, features, classes).")
                shap_values_for_importance = shap_values[:, :, 1] # Use class 1 slice for importance
                shap_values_for_plot = shap_values[:, :, 1]
                plot_title = "SHAP Summary Plot (Class 1 - Attack)"
            elif isinstance(shap_values, np.ndarray) and shap_values.ndim == 2 and shap_values.shape == (sample_size_shap, len(processed_feature_names_shap)):
                 # Handles case where output might be just for the positive class already
                 print("SHAP values format: Single array (samples, features) - assuming positive class.)")
                 shap_values_for_importance = shap_values
                 shap_values_for_plot = shap_values
                 plot_title = "SHAP Summary Plot (Assumed Positive Class)"
            else:
                print(f"Error: Unexpected SHAP values structure. Shape: {np.shape(shap_values)}")
                shap_values_for_importance = None


            if shap_values_for_importance is not None:
                # Ensure we have a 2D array (samples, features) before calculating mean
                if shap_values_for_importance.ndim == 2 and shap_values_for_importance.shape[1] == len(processed_feature_names_shap):
                    mean_abs_shap = np.abs(shap_values_for_importance).mean(axis=0)

                    if mean_abs_shap.ndim == 1 and len(mean_abs_shap) == len(processed_feature_names_shap):
                        feature_importance = pd.Series(mean_abs_shap, index=processed_feature_names_shap)
                        feature_importance_sorted = feature_importance.sort_values(ascending=False)

                        print("\n=== Mean Absolute SHAP Values (Feature Importance) ===")
                        print(feature_importance_sorted)

                        print("\n=== Top 10 Most Important Features ===")
                        print(feature_importance_sorted.head(10))
                    else:
                         print(f"Error: Could not create feature importance Series. Mean absolute SHAP shape: {mean_abs_shap.shape}, Number of feature names: {len(processed_feature_names_shap)}")
                else:
                    print(f"Error: SHAP values for importance calculation have unexpected shape: {shap_values_for_importance.shape}")

            else:
                print("Error: Failed to extract SHAP values for importance calculation.")
            # -------------------------------------------------

            # 6. Generate SHAP summary plot
            print(f"\n=== {plot_title} ===")
            if shap_values_for_plot is not None:
                 try:
                     # Check if plot_values has the expected structure for summary_plot
                     if isinstance(shap_values_for_plot, np.ndarray) and shap_values_for_plot.shape == (sample_size_shap, len(processed_feature_names_shap)):
                          shap.summary_plot(shap_values_for_plot, X_sample_shap, show=False)
                          plt.title(plot_title)
                          plt.show()
                     else:
                          print(f"Warning: SHAP values for plotting have unexpected shape {getattr(shap_values_for_plot, 'shape', 'N/A')}. Skipping summary plot.")

                 except Exception as plot_err:
                      print(f"Error generating SHAP summary plot: {plot_err}")
            else:
                 print("Error: SHAP values for plotting are not available. Skipping summary plot.")


        else:
            print("Not enough samples to calculate SHAP values.")

    except Exception as e:
        print(f"An error occurred during SHAP analysis: {e}")
        import traceback
        traceback.print_exc()
else:
    print("Best RandomForest model ('best_model'), test data, or preprocessor not available. Skipping SHAP analysis.")
