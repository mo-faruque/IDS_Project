%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath,amsfonts}
\usepackage{booktabs}
\usepackage{array}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\renewcommand{\abstractname}{Motivation}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Cybersecurity Intrusion Detection 
}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

\author{Aditya Jadhav}
\affiliation{%
  \institution{New Mexico State University}
  \city{Las Cruces}
  \country{USA}}
% \email{larst@affiliation.org}

\author{Nikshith Kilaru}
\affiliation{%
  \institution{New Mexico State University}
  \city{Las Cruces}
  \country{USA}}

\author{Dinesh Kumar Gowd Jadapalli}
\affiliation{%
  \institution{New Mexico State University}
  \city{Las Cruces}
  \country{USA}}

\author{Siva Sai Kumar Boina}
\affiliation{%
  \institution{New Mexico State University}
  \city{Las Cruces}
  \country{USA}}

\author{Md Omar Faruque}
\affiliation{%
  \institution{New Mexico State University}
  \city{Las Cruces}
  \country{USA}}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Attackers are using cutting-edge methods to get beyond conventional security measures, making cybersecurity threats more complex. Unauthorized access, phishing, and network penetration efforts present serious threats to both individuals and enterprises. Conventional rule-based intrusion detection systems (IDS) sometimes have trouble keeping up with changing attack patterns, which can result in false positives or threats being unnoticed.
By automatically detecting malicious activity based on real-time network behavior, machine learning (ML) offers a possible Solution. ML-based intrusion detection systems (IDS) may identify abnormalities that point to possible cyberthreats by learning attack patterns from past data, as opposed to depending on static rules.
Developing a strong machine learning (ML)-based intrusion detection system that can reliably identify network sessions as malicious or regular is essential given the growing frequency and complexity of intrusions.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Intrusion Detection, Machine learning, Explanatory Data Analysis}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Related Work}
Intrusion Detection Systems (IDS) are fundamental components in modern cybersecurity, providing mechanisms to detect, prevent, and respond to network-based attacks. With the increasing sophistication of cyber threats, machine learning (ML)-based IDS has gained attention due to its ability to analyze network behavior and detect anomalies. This section explores various research efforts that have contributed to the development of ML-driven IDS.
\subsection{Machine Learning Approaches for Intrusion Detection}
Early intrusion detection methods relied on \textbf{signature-based and rule-based detection}, which were effective for known attacks but failed against zero-day threats. To address this, ML-based IDS has been developed using classification and anomaly detection techniques. 
Decision Trees (DT), Support Vector Machines (SVM), Random Forest (RF), Naïve Bayes (NB), and K-Nearest Neighbors (KNN) have been widely used to classify network activities into normal and malicious categories. Research by  Indra et al. \cite{indra2024ensemble} demonstrated the effectiveness of ensemble learning techniques (RF + Gradient Boosting) in detecting novel threats with higher accuracy.
Unsupervised models like Isolation Forest and One-Class SVM have been explored for detecting novel cyber threats based on deviations from normal traffic patterns\cite{tavallaee2009detailed}. ML-based IDS models have shown improved detection rates compared to traditional methods, particularly when dealing with variant attack strategies.

\subsection{Deep Learning for Anomaly Detection in Cybersecurity}
Deep Learning (DL) techniques have further enhanced IDS performance by extracting complex patterns from vast amounts of network traffic data. Some notable contributions are provided below. 
Convolutional Neural Networks (RNN) and LSTM. Kim et al. \cite{9889698} explored Long Short-Term Memory (LSTM) networks for detecting temporal anomalies in network traffic, achieving high accuracy in detecting Denial-of-Service (DoS) attacks.
Convolutional Neural Networks (CNN) have been used to analyze network traffic in image-like formats, improving detection of malware-injected payloads and encrypted traffic attacks \cite{alsamiri2023federated}. 
Studies have applied autoencoders to learn compact representations of normal traffic, flagging deviations as potential intrusions \cite{molnar2020interpretable}.  
Deep learning models offer superior generalization but require large labeled datasets and extensive computational resources.


\subsection{Feature Engineering and Selection in Intrusion Detection}
Feature selection is crucial for enhancing IDS efficiency by reducing dimensionality while maintaining detection accuracy. Researchers have focused on Selecting Key Features. Network features like packet size, session duration, login attempts, encryption method, and IP reputation score have been identified as highly discriminative for ML-based IDS \cite{9887796}. PCA (Principal Component Analysis), Recursive Feature Elimination (RFE), and Mutual Information (MI) techniques have been used to eliminate redundant features and improve IDS interpretability \cite{shiravi2012toward}. By selecting optimal feature subsets, researchers have improved intrusion detection accuracy while reducing computational overhead.

\subsection{Benchmark Datasets for Intrusion Detection Research}
The effectiveness of an IDS heavily depends on the dataset used for training and evaluation. Several publicly available datasets have been used in IDS research, including:

\textbf{NSL-KDD:} An improvement over the KDD Cup 99 dataset, addressing class imbalance issues.

\textbf{CICIDS2017 \& UNSW-NB15:} These datasets provide realistic attack scenarios, including botnets, phishing, and DoS attacks \cite{10.1145/3658644.3691410}.

\textbf{Kaggle Cybersecurity Intrusion Detection Dataset:} The dataset used in this project provides labeled network activities with key features for intrusion detection \cite{kumar2023cybersecurity}.

These datasets serve as benchmarks for developing and testing ML-based IDS models.

\subsection{Hybrid Intrusion Detection Systems (HIDS)}
Recent research has proposed hybrid models that combine anomaly-based and signature-based methods to improve detection.IDS Hybrid IDS enhances detection rates by integrating rule-based methods with AI-driven anomaly detection \cite{indra2024ensemble}. Researchers have also fused CNNs with LSTM or Transformer-based architectures to analyze spatial and temporal features in network traffic \cite{9889698}. Hybrid IDS is gaining popularity for real-time security monitoring in enterprise networks.

\subsection{Adversarial Machine Learning and IDS Robustness}
Attackers have begun leveraging adversarial machine learning techniques to evade IDS detection. Research has investigated: Evasion Attacks: GANs (Generative Adversarial Networks) can generate malicious traffic patterns that bypass traditional ML-based IDS \cite{9887796}. Defense Mechanisms: Adversarial training, input preprocessing, and model hardening techniques are being explored to mitigate these attacks \cite{alsamiri2023federated}.

Ensuring IDS robustness against adversarial threats is critical for deployment in real-world cybersecurity scenarios.

\subsection{Explainable AI (XAI) for Intrusion Detection}
One challenge with ML-based IDS is lack of interpretability. Explainable AI (XAI) techniques aim to make intrusion detection more transparent. SHAP (SHapley Additive Explanations) and LIME (Local Interpretable Model-Agnostic Explanations) provide insights into how IDS models classify traffic \cite{molnar2020interpretable}. Some researchers have proposed hybrid models that integrate ML/DL models with decision trees to increase interpretability \cite{shiravi2012toward}. XAI enhances trust and adoption of AI-based IDS in security operations centers (SOCs).

% \subsection{Future Directions in Cybersecurity IDS }
% While ML-based IDS has made significant progress, challenges remain:  

%  \begin{itemize}
%   \item Reducing False Positives: False alarms are a major issue in real-world deployments.
%   \item Real-Time Processing: Many ML-based IDS models struggle with real-time traffic analysis due to high computational costs.
%   \item Federated Learning for IDS: Distributed and privacy-preserving IDS models are being explored to enhance security without sharing sensitive data.  
%   \item Intrusion Detection for IoT and Cloud Networks: Lightweight IDS for resource-constrained environments is a growing research area [15]. 
% \end{itemize}

\section{Problem Definition}
As cyber threats continue to evolve, traditional Intrusion Detection Systems (IDS) face significant limitations in detecting sophisticated attacks. Conventional rule-based IDS rely on predefined attack signatures and heuristic rules, which make them ineffective against zero-day attacks, polymorphic malware, and advanced persistent threats (APT). These systems also tend to generate a high number of false positives and fail to adapt to new attack strategies.

With the increasing complexity of cyberattacks, there is a growing need for intelligent, adaptive, and scalable intrusion detection systems capable of identifying anomalies in real time. Machine learning (ML)-based IDS have emerged as a potential solution by leveraging behavioral analysis and pattern recognition to detect both known and unknown threats. However, several challenges persist in developing an effective ML-based IDS:

\begin{itemize}
   \item \textbf{High False Positive and False Negative Rates} -- Many ML models struggle with balancing detection accuracy and minimizing false alerts.
   \item \textbf{Feature Selection and Dimensionality Reduction} -- Network traffic data is high-dimensional, requiring efficient techniques to extract the most relevant features.
   \item \textbf{Real-Time Detection Constraints} -- Most ML and deep learning (DL)-based IDS require significant computational resources, making real-time processing difficult.
   \item \textbf{Evasion Techniques} -- Attackers are increasingly using adversarial machine learning techniques to bypass IDS.
   \item \textbf{Scalability and Adaptability} -- IDS must handle large-scale network traffic and adapt to evolving cyber threats across different environments, including cloud and IoT networks.
\end{itemize}

Given these challenges, the goal of this project is to develop an ML-based Intrusion Detection System (IDS) that can effectively detect malicious network activities while minimizing false positives, optimizing feature selection, and enhancing real-time detection capabilities. The system will leverage supervised and unsupervised ML techniques to classify network sessions as either normal or malicious, using benchmark cybersecurity datasets for training and evaluation.

By addressing these challenges, this project aims to contribute to the development of a robust, efficient, and explainable IDS capable of enhancing cybersecurity defenses against evolving threats.

\section{Dataset Collection and Analysis}
We have collected two complementary datasets to ensure thorough testing of our intrusion detection algorithms.
\subsection{Dataset 1: UNSW-NB15}
The UNSW-NB15 dataset was created by the Cyber Range Lab of the Australian Centre for Cyber Security (ACCS).
\subsubsection{Source}
The dataset is available at \url{https://research.unsw.edu.au/projects/unsw-nb15-dataset}.
\subsubsection{Dataset Structure}
The dataset consists of 49 features and includes both the raw network packets and the processed features that can be used for machine learning models.
\begin{table}[h]
\caption{UNSW-NB15 Dataset Overview}
\centering
\begin{tabular}{p{2.5cm}p{5cm}}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Format & CSV \\
Total Instances & 175,341 training, 82,332 testing \\
Features & 49 (including the label) \\
Attack Types & 9 different attack categories \\
Size & 100MB (split into training and testing) \\
\bottomrule
\end{tabular}
\end{table}
\subsubsection{Key Characteristics}
The UNSW-NB15 dataset contains a mix of normal traffic and synthetic attack behaviors, making it representative of real-world network environments. The dataset includes features like flow duration, protocol type, and service type, along with derived statistical features.
Fig. 1 shows the correlation matrix of numerical features
in this dataset. 
\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{1.png}
\caption{Correlation matrix of numerical features in UNSW-NB15}
\end{figure}
\subsection{Dataset 2: Cybersecurity IDS Dataset (Kaggle)}
\subsubsection{Source}
This dataset is available at \url{https://www.kaggle.com/datasets/dnkumars/cybersecurity-intrusion-detection-dataset/}.
\subsubsection{Dataset Structure}
This dataset contains various network traffic features with binary classification labels.

\begin{table}[h]
\caption{Cybersecurity IDS Dataset Overview}
\centering
\begin{tabular}{p{2.5cm}p{5cm}}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Format & CSV \\
Total Instances & 9,537 \\
Features & 10 features + 1 target variable \\
Feature Types & 3 categorical, 7 numerical \\
Label & Binary (attack\_detected) \\
Size & Small (< 1MB) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Key Characteristics}
This dataset focuses on modern network traffic patterns and contains a good mix of categorical and numerical features. It provides a complementary perspective to the UNSW-NB15 dataset and helps validate the generalizability of our models.
Fig. 2 shows the correlation matrix of numerical features in this dataset, revealing relationships between different network properties.
\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{1.1.png}
\caption{Correlation matrix of numerical features in the Kaggle IDS dataset}
\end{figure}

% \subsection{Dataset Suitability Analysis}
% Both datasets are suitable for our IDS development for the following reasons:
% \begin{itemize}
% \item \textbf{Variety}: They contain diverse attack types and network behaviors
% \item \textbf{Feature Richness}: Both provide comprehensive network traffic attributes
% \item \textbf{Labeled Data}: Clear classification labels support supervised learning
% \item \textbf{Realistic Traffic}: They include patterns representative of real-world networks
% \item \textbf{Complementary Coverage}: Using two datasets ensures model robustness
% \end{itemize}

\section{Specific analysis tasks}
% Our team proposes an analytical approach to develop an accurate machine-learning model for predicting Intrusion Detection. We would like to perform the following tasks in our project. 

\subsection{Exploratory Data Analysis}

We performed a comprehensive exploratory data analysis (EDA) on both the UNSW-NB15 and Kaggle Cybersecurity IDS datasets to understand feature behavior, detect anomalies, and guide subsequent preprocessing and modeling steps.

We first examined the distribution of numerical features such as \texttt{dur}, \texttt{sbytes}, \texttt{dbytes}, \texttt{rate}, and \texttt{ct\_state\_ttl} using histograms and boxplots. These visualizations revealed several features exhibiting right-skewed distributions and extreme outliers, particularly in session duration (\texttt{dur}) and byte-related metrics.

We then analyzed relationships between features using Pearson correlation matrices. Highly correlated features (correlation $>$ 0.9) were flagged for potential removal to reduce redundancy. Notably, \texttt{sbytes} and \texttt{sttl} showed moderate positive correlation, and \texttt{dbytes} was correlated with \texttt{dpkts}, indicating duplication in traffic volume metrics.

Class distribution analysis showed that the target variable --- labeled as \texttt{label} in UNSW-NB15 and \texttt{attack\_detected} in the Kaggle dataset --- was highly imbalanced. Attack samples accounted for only about 25--30\% of the total instances. This imbalance motivated our decision to use resampling techniques during preprocessing.

Finally, categorical features such as \texttt{proto}, \texttt{service}, and \texttt{state} were analyzed using bar plots. We observed a strong dominance of TCP protocol and the \texttt{CON} state, suggesting protocol-level biases that might influence model learning. These findings influenced later steps in both feature encoding and feature engineering.


\subsection{Data Cleaning}

To ensure the quality of the training data, we implemented multiple data cleaning steps. Missing values were detected using \newline \texttt{df.isnull().sum()} and were found primarily in features such as \texttt{ct\_ftp\_cmd} and \texttt{ct\_flw\_http\_mthd}. Since these missing values occurred in a small fraction of the data and those columns were not strongly correlated with the target label, we dropped them entirely to prevent noisy input to the model.

Numerical outliers, particularly in \texttt{dur}, \texttt{sbytes}, and \texttt{rate}, were identified using the interquartile range (IQR) method. Values lying outside 1.5$\times$\texttt{IQR} were clipped to the upper or lower thresholds. This reduced the variance of high-impact features and stabilized model training without distorting the core data distribution.

We also cleaned categorical values by standardizing inconsistent strings (e.g., mapping null or unknown states to a common \texttt{UNKNOWN} label). Duplicate records were removed using \newline \texttt{df.duplicated().sum()} and \texttt{df.drop\_duplicates()}, which eliminated around 1.2\% of the Kaggle dataset, improving training reliability.

All preprocessing decisions were validated through rechecking feature distributions and correlation matrices. The cleaned datasets were saved separately to ensure reproducibility.



\subsection{Feature Engineering}

We engineered several new features aimed at improving the model’s ability to detect subtle attack patterns. One important derived feature was the \textbf{failed login ratio}, calculated as the ratio of failed login attempts to total login attempts. This captured authentication misuse more effectively than using either count in isolation.

Another major addition was the \textbf{security risk score}, which combined normalized values of \texttt{ip\_reputation\_score} and encryption strength. We assigned higher weights to IPs with bad reputations and flows using weak or outdated encryption (e.g., DES). This composite score was particularly useful for flagging high-risk flows that weren’t caught by traditional volume-based metrics.

Protocol-specific risk indicators were also created. Based on EDA findings, traffic using UDP with DES encryption or FTP service showed a disproportionate number of attacks. We created binary flags for these risky protocol-service combinations.

Lastly, we introduced time-based segmentation by binning \texttt{dur} into short, medium, and long sessions. This helped the model differentiate between low-duration scans and longer command-and-control sessions.

To evaluate the contribution of these features, we applied SHAP (SHapley Additive Explanations) to the tuned Random Forest model trained on the UNSW-NB15 dataset. SHAP analysis revealed that \texttt{rate}, \texttt{sbytes}, and \texttt{dur} were among the most influential features for classifying attacks. Features that contributed minimally or introduced multicollinearity were considered for removal in later refinements.




 \subsection{Dimensionality Reduction}

We applied Principal Component Analysis (PCA) to the standardized numerical features to reduce dimensionality and identify latent structure in the data. PCA helped isolate the top 10 components, which retained over 85\% of the original variance. This transformation reduced model complexity and was especially beneficial for exploratory analysis and runtime reduction.

% Additionally, we used t-distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP) to visualize class separation in two-dimensional space. These tools proved useful for interpreting the underlying data structure, although they were not used in final model training due to their non-deterministic nature and high computational cost.

PCA was evaluated within a Random Forest pipeline. While PCA slightly reduced training time, it did not improve classification performance. Thus, we retained PCA mainly for visual analytics and used full-dimensional data for final supervised learning models.




\subsection{Feature Selection}

To refine the input feature space and enhance model interpretability, we performed both filter-based and wrapper-based feature selection techniques.

% For filtering, we used chi-square tests for categorical features and ANOVA F-tests for numerical features. These statistical tests helped eliminate features that were not significantly associated with the target variable.

% In the wrapper-based approach, we implemented Recursive Feature Elimination (RFE) with cross-validation, using Random Forest as the base estimator. RFE iteratively removed the least important features and identified an optimal subset that contributed most to classification performance.

The most influential features across both selection techniques included \texttt{session\_duration}, \texttt{src\_bytes}, \texttt{packet\_size}, and \texttt{protocol\_type}. These findings aligned with SHAP value analysis, further confirming their importance in distinguishing malicious activity.

Feature selection ultimately improved model generalization, reduced dimensionality, and minimized overfitting risks—particularly in neural network models.




 \subsection{Preprocessing}

\begin{sloppypar}
We applied comprehensive preprocessing to prepare the datasets for modeling. \texttt{StandardScaler} was used to normalize numeric features such as \texttt{dur}, \texttt{rate}, and \texttt{sbytes}. This ensured consistent input ranges across features and improved convergence for algorithms like neural networks and PCA.

Categorical features such as \texttt{proto}, \texttt{service}, and \texttt{state} were encoded using \texttt{OneHotEncoder} within a \texttt{scikit-learn ColumnTransformer}. This transformation maintained compatibility with all machine learning models by converting categorical variables into a numeric format.

Due to significant class imbalance in both datasets (with only approximately 25--30\% of rows labeled as attacks), we applied SMOTE (Synthetic Minority Oversampling Technique) during neural network training. This oversampling strategy balanced the training set and reduced model bias.

All preprocessing steps were built into reproducible pipelines and validated through consistency checks, ensuring robust and clean data flow throughout the experiments.
\end{sloppypar}


% \subsection{Model Training and Selection}
% For model development, we will implement a machine-learning pipeline using supervised classification algorithms including decision trees, random forests, and gradient-boosting methods like XGBoost. We'll employ systematic cross-validation to tune hyperparameters, finding the optimal balance between model complexity and generalization ability. By comparing various accuracy metrics across both training and validation datasets, we'll identify the most robust model that can reliably distinguish between normal network traffic and malicious activity.

% \subsection{Evaluation}
% We will prioritize precision, recall, and F1-score over accuracy alone, as false negatives (missed attacks) are typically more costly than false positives in cybersecurity. We'll generate confusion matrices to analyze specific types of errors and create precision-recall curves to evaluate performance across different threshold settings. We'll conduct error analysis on misclassified instances to identify patterns and guide further feature engineering improvements.

Our team proposes an analytical approach to develop an accurate machine-learning model for predicting Intrusion Detection. We would like to perform the following tasks in our project. 


\subsection{Model Training and Selection}

To develop an effective intrusion detection system (IDS), we implemented and compared multiple supervised classification algorithms: Random Forest, Artificial Neural Network (ANN), and Random Forest with PCA. Additionally, we experimented with Fuzzy C-Means clustering for unsupervised learning and used \texttt{LazyClassifier} for preliminary benchmarking.

We used a stratified 80/20 train-test split to preserve the target class distribution. For model development, a preprocessing pipeline was created using \texttt{ColumnTransformer}, which included \texttt{StandardScaler} for numeric features and \texttt{OneHotEncoder} for categorical features. This pipeline was applied across all models to ensure consistency.

\textbf{Random Forest with GridSearchCV:} We selected Random Forest for its robustness and interpretability. A pipeline combining preprocessing and \texttt{RandomForestClassifier} was tuned using \texttt{GridSearchCV} with 5-fold cross-validation. The following hyperparameters were optimized:
\begin{itemize}
    \item \texttt{n\_estimators}: 100, 200, 300
    \item \texttt{max\_depth}: None, 10, 20, 30
\end{itemize}
The best model was obtained with \texttt{n\_estimators = 200} and \texttt{max\_depth = 20}, achieving a cross-validation accuracy of 91.8\%.

\textbf{ANN with SMOTE:} To handle class imbalance, we applied SMOTE to the training data. The ANN was constructed with three dense layers (64 $\rightarrow$ 32 $\rightarrow$ 1) and included dropout regularization. Early stopping was used to prevent overfitting. The ANN model demonstrated competitive accuracy and generalization, particularly for the minority class.

\textbf{PCA with Random Forest:} To assess the impact of dimensionality reduction, PCA (with $n=10$ components) was added before Random Forest in the pipeline. While PCA reduced training time and noise, it caused a slight drop in accuracy. However, interpretability improved through feature visualization and SHAP analysis.

\textbf{Fuzzy C-Means Clustering:} Although our primary focus was supervised learning, we included Fuzzy C-Means as a baseline unsupervised method. It was evaluated using different values of $c$ (number of clusters) and $m$ (fuzzifier exponent). The best accuracy of approximately 79\% was achieved with $c = 2$ and $m = 2.0$, though performance lagged behind the supervised models.

\textbf{SHAP Explainability:} SHAP was applied to interpret the best-performing Random Forest model. It revealed key influential features such as \texttt{sbytes}, \texttt{dbytes}, and \texttt{service} types, offering transparency and actionable insights for cybersecurity professionals.


\subsection{Evaluation}

We prioritized F1-score, precision, and recall over raw accuracy due to the high cost of false negatives in an IDS context. All models were evaluated on a consistent test set to ensure fairness across comparisons.

\begin{table}[h]
\centering
\small
\caption{Model Performance Comparison for  Cybersecurity IDS Dataset}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Random Forest (Tuned) & 91.4\% & 91.7\% & 91.2\% & 91.4\% \\
ANN + SMOTE & 89.5\% & 90.2\% & 88.8\% & 89.5\% \\
PCA + RF & 88.3\% & 88.5\% & 88.0\% & 88.2\% \\
Fuzzy C-Means & 79.1\% & 78.6\% & 79.4\% & 79.0\% \\
\bottomrule
\end{tabular}
\end{table}

% \begin{table}[h]
% \centering
% \caption{Model Performance Comparison for Cybersecurity IDS Dataset}
% \begin{tabular}{@{}lcccc@{}}
% \toprule
% \textbf{Model} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} \\
% \midrule
% Random Forest (Tuned) & 88.5\% & 91.5\% & 87.0\% & 88.0\% \\
% ANN + SMOTE & 83.7\% & 83.5\% & 83.0\% & 83.5\% \\
% PCA + RF & 82.0\% & 83.5\% & 81.5\% & 82.0\% \\
% Fuzzy C-Means & 68.0\% & 69.5\% & 69.0\% & 68.0\% \\
% \bottomrule
% \end{tabular}
% \end{table}

\textbf{Confusion Matrix Analysis:}  
Confusion matrices were used to further assess classification performance. The Random Forest model exhibited minimal false negatives, making it suitable for real-time intrusion detection. The ANN model showed better generalization on imbalanced classes, particularly after SMOTE oversampling, though it required longer training time. Fuzzy C-Means struggled to align clusters with actual attack labels, reflecting its limitations as an unsupervised approach in this context.

\textbf{ROC and Precision-Recall Curves:}  
We plotted ROC curves for all binary classifiers. The Random Forest model achieved an AUC (Area Under Curve) of 0.96, indicating excellent discrimination between attack and normal classes. Precision-Recall curves were also analyzed to understand performance across varying thresholds, especially under class imbalance.

\textbf{Final Model Selection:}  
Based on its balance of accuracy, low inference latency, and interpretability, the tuned Random Forest model was selected as the final model for deployment. The ANN model was retained as a secondary option for future work exploring deep learning architectures in IDS systems.
.

% \subsection{Adversarial Robustness}
% We will implement techniques to enhance model robustness against adversarial attacks, including adversarial training where we augment our dataset with perturbed examples that might evade detection. We'll explore ensemble methods that combine multiple models with different strengths to reduce vulnerability to specific attack vectors. We'll also implement anomaly detection as a complementary approach to our classification models, using techniques like Isolation Forest and One-Class SVM to identify traffic patterns that deviate significantly from established normal behavior, which may indicate novel or adversarial attacks.


\subsection{Real-Time Deployment Considerations}
We will analyze model inference latency  by benchmarking Random Forest, XGBoost and Neural Network models under . Memory optimization techniques will include decision tree pruning (limiting depth, reducing estimators), feature subset selection (top 10-20 features), and neural network quantization (16/8-bit representations). We'll quantify performance-accuracy tradeoffs through precision-recall vs. latency curves to identify optimal configurations that balance detection quality with computational efficiency.




% \section{Initial Code Implementation}
% We have implemented initial code for data analysis and preprocessing, focusing on:
% \subsection{Data Exploration and Visualization}
% Our code performs the following exploratory analysis:
% \begin{itemize}
% \item Basic statistics (mean, variance, etc.) for numerical features
% \item Distribution visualization using histograms and boxplots
% \item Correlation analysis between features
% \item Target class distribution analysis
% \end{itemize}
% \subsection{Preprocessing Pipeline}
% We've developed a scikit-learn preprocessing pipeline that handles:
% \begin{itemize}
% \item Missing value detection and reporting
% \item Feature scaling using StandardScaler
% \item Categorical feature encoding using OneHotEncoder
% \item Train-test splitting with stratification
% \end{itemize}
% \subsection{Initial Model Testing}
% Our code implements:
% \begin{itemize}
% \item LazyClassifier for quick model benchmarking
% \item Random Forest with hyperparameter tuning via GridSearchCV
% \item Dimensionality reduction using PCA
% \item Neural network implementation with dropout regularization
% \item Fuzzy C-Means clustering for unsupervised analysis
% \end{itemize}
% \begin{algorithm}
% \caption{Basic Random Forest Pipeline}
% \begin{algorithmic}[1]
% \STATE Define preprocessor for numeric and categorical features
% \STATE Split data into training and test sets
% \STATE Create RandomForest pipeline with preprocessor
% \STATE Define hyperparameter grid
% \STATE Perform GridSearchCV with 5-fold cross-validation
% \STATE Select best model based on accuracy
% \STATE Evaluate on test set and report metrics
% \end{algorithmic}
% \end{algorithm}


\section{Conclusion and Next Steps}
In Stage 4, we have:
\begin{itemize}
\item we refined preprocessing, performed hyperparameter tuning.
\item Addressed class imbalance using techniques like SMOTE 
\item  Implemented explainability via SHAP.
\item Evaluated models using ROC curves, precision-recall curves, and confusion matrices
\end{itemize}
Our next steps include the following.
\begin{itemize}
\item optimizing inference latency. 
\item Real-Time Deployment Analysis 
\end{itemize}


% \subsection{Template Styles}

% The primary parameter given to the ``\verb|acmart|'' document class is
% the {\itshape template style} which corresponds to the kind of publication
% or SIG publishing the work. This parameter is enclosed in square
% brackets and is a part of the {\verb|documentclass|} command:
% \begin{verbatim}
%   \documentclass[STYLE]{acmart}
% \end{verbatim}

% Journals use one of three template styles. All but three ACM journals
% use the {\verb|acmsmall|} template style:
% \begin{itemize}
% \item {\texttt{acmsmall}}: The default journal template style.
% \item {\texttt{acmlarge}}: Used by JOCCH and TAP.
% \item {\texttt{acmtog}}: Used by TOG.
% \end{itemize}

% The majority of conference proceedings documentation will use the {\verb|acmconf|} template style.
% \begin{itemize}
% \item {\texttt{sigconf}}: The default proceedings template style.
% \item{\texttt{sigchi}}: Used for SIGCHI conference articles.
% \item{\texttt{sigplan}}: Used for SIGPLAN conference articles.
% \end{itemize}

% \subsection{Template Parameters}

% In addition to specifying the {\itshape template style} to be used in
% formatting your work, there are a number of {\itshape template parameters}
% which modify some part of the applied template style. A complete list
% of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}

% Frequently-used parameters, or combinations of parameters, include:
% \begin{itemize}
% \item {\texttt{anonymous,review}}: Suitable for a ``double-anonymous''
%   conference submission. Anonymizes the work and includes line
%   numbers. Use with the \texttt{\string\acmSubmissionID} command to print the
%   submission's unique ID on each page of the work.
% \item{\texttt{authorversion}}: Produces a version of the work suitable
%   for posting by the author.
% \item{\texttt{screen}}: Produces colored hyperlinks.
% \end{itemize}

% This document uses the following string as the first command in the
% source file:
% \begin{verbatim}
% \documentclass[sigconf]{acmart}
% \end{verbatim}

% \section{Modifications}

% Modifying the template --- including but not limited to: adjusting
% margins, typeface sizes, line spacing, paragraph and list definitions,
% and the use of the \verb|\vspace| command to manually adjust the
% vertical spacing between elements of your work --- is not allowed.

% {\bfseries Your document will be returned to you for revision if
%   modifications are discovered.}

% \section{Typefaces}

% The ``\verb|acmart|'' document class requires the use of the
% ``Libertine'' typeface family. Your \TeX\ installation should include
% this set of packages. Please do not substitute other typefaces. The
% ``\verb|lmodern|'' and ``\verb|ltimes|'' packages should not be used,
% as they will override the built-in typeface families.

% \section{Title Information}

% The title of your work should use capital letters appropriately -
% \url{https://capitalizemytitle.com/} has useful rules for
% capitalization. Use the {\verb|title|} command to define the title of
% your work. If your work has a subtitle, define it with the
% {\verb|subtitle|} command.  Do not insert line breaks in your title.

% If your title is lengthy, you must define a short version to be used
% in the page headers, to prevent overlapping text. The \verb|title|
% command has a ``short title'' parameter:
% \begin{verbatim}
%   \title[short title]{full title}
% \end{verbatim}

% \section{Authors and Affiliations}

% Each author must be defined separately for accurate metadata
% identification.  As an exception, multiple authors may share one
% affiliation. Authors' names should not be abbreviated; use full first
% names wherever possible. Include authors' e-mail addresses whenever
% possible.

% Grouping authors' names or e-mail addresses, or providing an ``e-mail
% alias,'' as shown below, is not acceptable:
% \begin{verbatim}
%   \author{Brooke Aster, David Mehldau}
%   \email{dave,judy,steve@university.edu}
%   \email{firstname.lastname@phillips.org}
% \end{verbatim}

% The \verb|authornote| and \verb|authornotemark| commands allow a note
% to apply to multiple authors --- for example, if the first two authors
% of an article contributed equally to the work.

% If your author list is lengthy, you must define a shortened version of
% the list of authors to be used in the page headers, to prevent
% overlapping text. The following command should be placed just after
% the last \verb|\author{}| definition:
% \begin{verbatim}
%   \renewcommand{\shortauthors}{McCartney, et al.}
% \end{verbatim}
% Omitting this command will force the use of a concatenated list of all
% of the authors' names, which may result in overlapping text in the
% page headers.

% The article template's documentation, available at
% \url{https://www.acm.org/publications/proceedings-template}, has a
% complete explanation of these commands and tips for their effective
% use.

% Note that authors' addresses are mandatory for journal articles.

% \section{Rights Information}

% Authors of any work published by ACM will need to complete a rights
% form. Depending on the kind of work, and the rights management choice
% made by the author, this may be copyright transfer, permission,
% license, or an OA (open access) agreement.

% Regardless of the rights management choice, the author will receive a
% copy of the completed rights form once it has been submitted. This
% form contains \LaTeX\ commands that must be copied into the source
% document. When the document source is compiled, these commands and
% their parameters add formatted text to several areas of the final
% document:
% \begin{itemize}
% \item the ``ACM Reference Format'' text on the first page.
% \item the ``rights management'' text on the first page.
% \item the conference information in the page header(s).
% \end{itemize}

% Rights information is unique to the work; if you are preparing several
% works for an event, make sure to use the correct set of commands with
% each of the works.

% The ACM Reference Format text is required for all articles over one
% page in length, and is optional for one-page articles (abstracts).

% \section{CCS Concepts and User-Defined Keywords}

% Two elements of the ``acmart'' document class provide powerful
% taxonomic tools for you to help readers find your work in an online
% search.

% The ACM Computing Classification System ---
% \url{https://www.acm.org/publications/class-2012} --- is a set of
% classifiers and concepts that describe the computing
% discipline. Authors can select entries from this classification
% system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
% commands to be included in the \LaTeX\ source.

% User-defined keywords are a comma-separated list of words and phrases
% of the authors' choosing, providing a more flexible way of describing
% the research being presented.

% CCS concepts and user-defined keywords are required for for all
% articles over two pages in length, and are optional for one- and
% two-page articles (or abstracts).

% \section{Sectioning Commands}

% Your work should use standard \LaTeX\ sectioning commands:
% \verb|\section|, \verb|\subsection|, \verb|\subsubsection|,
% \verb|\paragraph|, and \verb|\subparagraph|. The sectioning levels up to
% \verb|\subsusection| should be numbered; do not remove the numbering
% from the commands.

% Simulating a sectioning command by setting the first word or words of
% a paragraph in boldface or italicized text is {\bfseries not allowed.}

% Below are examples of sectioning commands.

% \subsection{Subsection}
% \label{sec:subsection}

% This is a subsection.

% \subsubsection{Subsubsection}
% \label{sec:subsubsection}

% This is a subsubsection.

% \paragraph{Paragraph}

% This is a paragraph.

% \subparagraph{Subparagraph}

% This is a subparagraph.

% \section{Tables}

% The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
% package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
% high-quality tables.

% Table captions are placed {\itshape above} the table.

% Because tables cannot be split across pages, the best placement for
% them is typically the top of the page nearest their initial cite.  To
% ensure this proper ``floating'' placement of tables, use the
% environment \textbf{table} to enclose the table's contents and the
% table caption.  The contents of the table itself must go in the
% \textbf{tabular} environment, to be aligned properly in rows and
% columns, with the desired horizontal and vertical rules.  Again,
% detailed instructions on \textbf{tabular} material are found in the
% \textit{\LaTeX\ User's Guide}.

% Immediately following this sentence is the point at which
% Table~\ref{tab:freq} is included in the input file; compare the
% placement of the table here with the table in the printed output of
% this document.

% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}

% To set a wider table, which takes up the whole width of the page's
% live area, use the environment \textbf{table*} to enclose the table's
% contents and the table caption.  As with a single-column table, this
% wide table will ``float'' to a location deemed more
% desirable. Immediately following this sentence is the point at which
% Table~\ref{tab:commands} is included in the input file; again, it is
% instructive to compare the placement of the table here with the table
% in the printed output of this document.

% \begin{table*}
%   \caption{Some Typical Commands}
%   \label{tab:commands}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

% Always use midrule to separate table header rows from data rows, and
% use it only for this purpose. This enables assistive technologies to
% recognise table headers and support their users in navigating tables
% more easily.

% \section{Math Equations}
% You may want to display math equations in three distinct styles:
% inline, numbered or non-numbered display.  Each of the three are
% discussed in the next sections.

% \subsection{Inline (In-text) Equations}
% A formula that appears in the running text is called an inline or
% in-text formula.  It is produced by the \textbf{math} environment,
% which can be invoked with the usual
% \texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
% the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
% and structures, from $\alpha$ to $\omega$, available in
% \LaTeX~\cite{Lamport:LaTeX}; this section will simply show a few
% examples of in-text equations in context. Notice how this equation:
% \begin{math}
%   \lim_{n\rightarrow \infty}x=0
% \end{math},
% set here in in-line math style, looks slightly different when
% set in display style.  (See next section).

% \subsection{Display Equations}
% A numbered display equation---one set off by vertical space from the
% text and centered horizontally---is produced by the \textbf{equation}
% environment. An unnumbered display equation is produced by the
% \textbf{displaymath} environment.

% Again, in either environment, you can use any of the symbols and
% structures available in \LaTeX\@; this section will just give a couple
% of examples of display equations in context.  First, consider the
% equation, shown as an inline equation above:
% \begin{equation}
%   \lim_{n\rightarrow \infty}x=0
% \end{equation}
% Notice how it is formatted somewhat differently in
% the \textbf{displaymath}
% environment.  Now, we'll enter an unnumbered equation:
% \begin{displaymath}
%   \sum_{i=0}^{\infty} x + 1
% \end{displaymath}
% and follow it with another numbered equation:
% \begin{equation}
%   \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
% \end{equation}
% just to demonstrate \LaTeX's able handling of numbering.

% \section{Figures}

% The ``\verb|figure|'' environment should be used for figures. One or
% more images can be placed within a figure. If your figure contains
% third-party material, you must clearly identify it as such, as shown
% in the example below.
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{sample-franklin}
%   \caption{1907 Franklin Model D roadster. Photograph by Harris \&
%     Ewing, Inc. [Public domain], via Wikimedia
%     Commons. (\url{https://goo.gl/VLCRBB}).}
%   \Description{A woman and a girl in white dresses sit in an open car.}
% \end{figure}

% Your figures should contain a caption which describes the figure to
% the reader.

% Figure captions are placed {\itshape below} the figure.

% Every figure should also have a figure description unless it is purely
% decorative. These descriptions convey what’s in the image to someone
% who cannot see it. They are also used by search engine crawlers for
% indexing images, and when images cannot be loaded.

% A figure description must be unformatted plain text less than 2000
% characters long (including spaces).  {\bfseries Figure descriptions
%   should not repeat the figure caption – their purpose is to capture
%   important information that is not already provided in the caption or
%   the main text of the paper.} For figures that convey important and
% complex new information, a short text description may not be
% adequate. More complex alternative descriptions can be placed in an
% appendix and referenced in a short figure description. For example,
% provide a data table capturing the information in a bar chart, or a
% structured list representing a graph.  For additional information
% regarding how best to write figure descriptions and why doing this is
% so important, please see
% \url{https://www.acm.org/publications/taps/describing-figures/}.

% \subsection{The ``Teaser Figure''}

% A ``teaser figure'' is an image, or set of images in one figure, that
% are placed after all author and affiliation information, and before
% the body of the article, spanning the page. If you wish to have such a
% figure in your article, place the command immediately before the
% \verb|\maketitle| command:
% \begin{verbatim}
%   \begin{teaserfigure}
%     \includegraphics[width=\textwidth]{sampleteaser}
%     \caption{figure caption}
%     \Description{figure description}
%   \end{teaserfigure}
% \end{verbatim}

% \section{Citations and Bibliographies}

% The use of \BibTeX\ for the preparation and formatting of one's
% references is strongly recommended. Authors' names should be complete
% --- use full first names (``Donald E. Knuth'') not initials
% (``D. E. Knuth'') --- and the salient identifying features of a
% reference should be included: title, year, volume, number, pages,
% article DOI, etc.

% The bibliography is included in your source document with these two
% commands, placed just before the \verb|\end{document}| command:
% \begin{verbatim}
%   \bibliographystyle{ACM-Reference-Format}
%   \bibliography{bibfile}
% \end{verbatim}
% where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
% suffix, of the \BibTeX\ file.

% Citations and references are numbered by default. A small number of
% ACM publications have citations and references formatted in the
% ``author year'' style; for these exceptions, please include this
% command in the {\bfseries preamble} (before the command
% ``\verb|\begin{document}|'') of your \LaTeX\ source:
% \begin{verbatim}
%   \citestyle{acmauthoryear}
% \end{verbatim}


%   Some examples.  A paginated journal article \cite{Abril07}, an
%   enumerated journal article \cite{Cohen07}, a reference to an entire
%   issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
%   monograph/whole book in a series (see 2a in spec. document)
%   \cite{Harel79}, a divisible-book such as an anthology or compilation
%   \cite{Editor00} followed by the same example, however we only output
%   the series if the volume number is given \cite{Editor00a} (so
%   Editor00a's series should NOT be present since it has no vol. no.),
%   a chapter in a divisible book \cite{Spector90}, a chapter in a
%   divisible book in a series \cite{Douglass98}, a multi-volume work as
%   book \cite{Knuth97}, a couple of articles in a proceedings (of a
%   conference, symposium, workshop for example) (paginated proceedings
%   article) \cite{Andler79, Hagerup1993}, a proceedings article with
%   all possible elements \cite{Smith10}, an example of an enumerated
%   proceedings article \cite{VanGundy07}, an informally published work
%   \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
%     AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
%   master's thesis: \cite{anisi03}, an online document / world wide web
%   resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
%   (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
%   and (Case 3) a patent \cite{JoeScientist001}, work accepted for
%   publication \cite{rous08}, 'YYYYb'-test for prolific author
%   \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
%   contain 'duplicate' DOI and URLs (some SIAM articles)
%   \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
%   multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
%   couple of citations with DOIs:
%   \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
%   citations: \cite{TUGInstmem, Thornburg01, CTANacmart}.
%   Artifacts: \cite{R} and \cite{UMassCitations}.

% \section{Acknowledgments}

% Identification of funding sources and other support, and thanks to
% individuals and groups that assisted in the research and the
% preparation of the work should be included in an acknowledgment
% section, which is placed just before the reference section in your
% document.

% This section has a special environment:
% \begin{verbatim}
%   \begin{acks}
%   ...
%   \end{acks}
% \end{verbatim}
% so that the information contained therein can be more easily collected
% during the article metadata extraction phase, and to ensure
% consistency in the spelling of the section heading.

% Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

% \section{Appendices}

% If your work needs an appendix, add it before the
% ``\verb|\end{document}|'' command at the conclusion of your source
% document.

% Start the appendix with the ``\verb|appendix|'' command:
% \begin{verbatim}
%   \appendix
% \end{verbatim}
% and note that in the appendix, sections are lettered, not
% numbered. This document has two appendices, demonstrating the section
% and subsection identification method.

% \section{Multi-language papers}

% Papers may be written in languages other than English or include
% titles, subtitles, keywords and abstracts in different languages (as a
% rule, a paper in a language other than English should include an
% English title and an English abstract).  Use \verb|language=...| for
% every language used in the paper.  The last language indicated is the
% main language of the paper.  For example, a French paper with
% additional titles and abstracts in English and German may start with
% the following command
% \begin{verbatim}
% \documentclass[sigconf, language=english, language=german,
%                language=french]{acmart}
% \end{verbatim}

% The title, subtitle, keywords and abstract will be typeset in the main
% language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
% begin title, subtitle and keywords, can be used to set these elements
% in the other languages.  The environment \verb|translatedabstract| is
% used to set the translation of the abstract.  These commands and
% environment have a mandatory first argument: the language of the
% second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
% of their usage.

% \section{SIGCHI Extended Abstracts}

% The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
% not in Word) produces a landscape-orientation formatted article, with
% a wide left margin. Three environments are available for use with the
% ``\verb|sigchi-a|'' template style, and produce formatted output in
% the margin:
% \begin{description}
% \item[\texttt{sidebar}:]  Place formatted text in the margin.
% \item[\texttt{marginfigure}:] Place a figure in the margin.
% \item[\texttt{margintable}:] Place a table in the margin.
% \end{description}

% %%
% %% The acknowledgments section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata, and the
% %% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
